{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE474/574 - Programming Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Sentiment Analysis\n",
    "\n",
    "In the code provided below, you need to add code wherever specified by `TODO:`. \n",
    "\n",
    "> You will be using a Python collection class - `Counter` to maintain the word counts. \n",
    "\n",
    "> See https://docs.python.org/2/library/collections.html for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data files \n",
    "g = open('reviews.txt','r') # What we know!\n",
    "reviews_all = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "g = open('labels.txt','r') # What we WANT to know!\n",
    "sentiments_all = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()\n",
    "\n",
    "# load vocabulary\n",
    "g = open('vocab.txt','r')\n",
    "vocab = [s.strip() for s in g.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a set of 25000 movie reviews, along with a `POSITIVE` or `NEGATIVE` sentiment label assigned to the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A POSITIVE review:\n",
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "\n",
      "A NEGATIVE review:\n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n"
     ]
    }
   ],
   "source": [
    "# Check out sample reviews\n",
    "print('A {} review:'.format(sentiments_all[0]))\n",
    "print(reviews_all[0])\n",
    "print('\\nA {} review:'.format(sentiments_all[1]))\n",
    "print(reviews_all[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and test data\n",
    "reviews_train,reviews_test = reviews_all[0:24000],reviews_all[24000:]\n",
    "sentiments_train,sentiments_test = sentiments_all[0:24000],sentiments_all[24000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maintain Counter objects to store positive, negative and total counts for\n",
    "# all the words present in the positive, negative and total reviews.\n",
    "positive_word_count = Counter()\n",
    "negative_word_count = Counter()\n",
    "total_counts = Counter()\n",
    "# TODO: Loop over all the words in the vocabulary\n",
    "# and increment the counts in the appropriate counter objects\n",
    "# based on the training data\n",
    "\n",
    "for i in range(len(reviews_train)):\n",
    "    review = reviews_train[i]\n",
    "    splits = review.split()\n",
    "    if(sentiments_train[i] == 'POSITIVE'):\n",
    "        for j in splits:\n",
    "            if(j in vocab):\n",
    "                positive_word_count[j] = positive_word_count[j] + 1;\n",
    "                total_counts[j] = total_counts[j] + 1;\n",
    "    else:\n",
    "        for k in splits:\n",
    "            if(k in vocab):\n",
    "                negative_word_count[k] = negative_word_count[k] + 1;\n",
    "                total_counts[k] = total_counts[k] + 1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maintain a Counter object to store positive to negative ratios \n",
    "pos_neg_ratios = Counter()\n",
    "\n",
    "# Calculate the ratios of positive and negative uses of the most common words\n",
    "# Consider words to be \"common\" if they've been used at least 100 times\n",
    "for term,cnt in list(total_counts.most_common()):\n",
    "    if(cnt > 100):\n",
    "        # TODO: Code for calculating the ratios (remove the next line)\n",
    "        if negative_word_count[term] != 0:\n",
    "            pos_neg_ratios[term] = positive_word_count[term] / negative_word_count[term];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos-to-neg ratio for 'the' = 1.0618582280413789\n",
      "Pos-to-neg ratio for 'amazing' = 4.031496062992126\n",
      "Pos-to-neg ratio for 'terrible' = 0.17256637168141592\n"
     ]
    }
   ],
   "source": [
    "print(\"Pos-to-neg ratio for 'the' = {}\".format(pos_neg_ratios[\"the\"]))\n",
    "print(\"Pos-to-neg ratio for 'amazing' = {}\".format(pos_neg_ratios[\"amazing\"]))\n",
    "print(\"Pos-to-neg ratio for 'terrible' = {}\".format(pos_neg_ratios[\"terrible\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a log of the ratio\n",
    "for word,ratio in pos_neg_ratios.most_common():\n",
    "    pos_neg_ratios[word] = np.log(ratio)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the distribution of the log-ratio scores\n",
    "scores = np.array(list(pos_neg_ratios.values()))\n",
    "vocab_selected = list(pos_neg_ratios.keys())\n",
    "\n",
    "h = plt.hist(scores,bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above histogram should give you an idea about the distribution of the scores.\n",
    "\n",
    "Notice how the scores are distributed around 0. A word with score 0 can be considered as `neutral`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realize\n",
      "hands\n",
      "extreme\n",
      "beat\n",
      "onto\n",
      "psycho\n",
      "test\n",
      "obsessed\n",
      "choose\n",
      "speech\n"
     ]
    }
   ],
   "source": [
    "# Print few words with neutral score\n",
    "for ind in np.where(scores == 0)[0][0:10]:\n",
    "    print(vocab_selected[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**APPROACH 1** Implement a simple non-machine learning that only uses the log-ratios to determine if a review is positive or negative. This function will be applied to the test data to calculate the accuracy of the model. \n",
    "\n",
    "_See the assignment document for hints._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonml_classifier(review,pos_neg_ratios):\n",
    "    '''\n",
    "    Function that determines the sentiment for a given review.\n",
    "    \n",
    "    Inputs:\n",
    "      review - A text containing a movie review\n",
    "      pos_neg_ratios - A Counter object containing frequent words\n",
    "                       and corresponding log positive-negative ratio\n",
    "    Return:\n",
    "      sentiment - 'NEGATIVE' or 'POSITIVE'\n",
    "    '''\n",
    "    # TODO: Implement the algorithm here. Change the next line.\n",
    "    splits = review.split();\n",
    "    count =0;\n",
    "    for i in range(len(splits)):\n",
    "        count = count + pos_neg_ratios[splits[i]]\n",
    "    \n",
    "    if count >= 0:\n",
    "        return 'POSITIVE'\n",
    "    \n",
    "    return 'NEGATIVE'\n",
    "    return 'NEGATIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model = 0.763\n"
     ]
    }
   ],
   "source": [
    "predictions_test = []\n",
    "for r in reviews_test:\n",
    "    l = nonml_classifier(r,pos_neg_ratios)\n",
    "    predictions_test.append(l)\n",
    "\n",
    "# calculate accuracy\n",
    "correct = 0\n",
    "for l,p in zip(sentiments_test,predictions_test):\n",
    "    if l == p:\n",
    "        correct = correct + 1\n",
    "print('Accuracy of the model = {}'.format(correct/len(sentiments_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 2** Implement a neural network for sentiment classification. \n",
    "\n",
    "> ### System Configuration\n",
    "This part requires you to use a computer with `tensorflow` library installed. More information is available here - https://www.tensorflow.org.\n",
    "`\n",
    "You are allowed to implement the project on your personal computers using `Python 3.4 or above. You will need `numpy` and `scipy` libraries. If you need to use departmental resources, you can use **metallica.cse.buffalo.edu**, which has `Python 3.4.3` and the required libraries installed. \n",
    "\n",
    "> Students attempting to use the `tensorflow` library have two options: \n",
    "1. Install `tensorflow` on personal machines. Detailed installation information is here - https://www.tensorflow.org/. Note that, since `tensorflow` is a relatively new library, you might encounter installation issues depending on your OS and other library versions. We will not be providing any detailed support regarding `tensorflow` installation. If issues persist, we recommend using option 2. \n",
    "2. Use **metallica.cse.buffalo.edu**. If you are registered into the class, you should have an account on that server. The server already has Python 3.4.3 and TensorFlow 0.12.1 installed. Please use /util/bin/python for Python 3. \n",
    "3. To maintain a ssh connection for a long-running task on a remote machine, use tools like `screen`. For more information: https://linuxize.com/post/how-to-use-linux-screen/ \n",
    "4. For running jupyter-notebook over a remote machine find information on: https://fizzylogic.nl/2017/11/06/edit-jupyter-notebooks-over-ssh/\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_vector(review,word2index):\n",
    "    '''\n",
    "    Function to count how many times each word is used in the given review,\n",
    "    # and then store those counts at the appropriate indices inside x.\n",
    "    '''\n",
    "    vocab_size = len(word2index)\n",
    "    x = np.zeros((1, vocab_size))\n",
    "    for w in review.split(' '):\n",
    "        if w in word2index.keys():\n",
    "            x[0][word2index[w]] += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ignore_words(pos_neg_ratios):\n",
    "    '''\n",
    "    Function to identify words to ignore from the vocabulary\n",
    "    '''\n",
    "    ignore_words = []\n",
    "    # TODO: Complete the implementation of find_ignore_words\n",
    "    scores = np.array(list(pos_neg_ratios.values()))\n",
    "    words = np.array(list(pos_neg_ratios.keys()))\n",
    "    for i in np.where(scores == 0)[0]:\n",
    "        ignore_words.append(words[i])\n",
    "    print(ignore_words)\n",
    "    return ignore_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['realize', 'hands', 'extreme', 'beat', 'onto', 'psycho', 'test', 'obsessed', 'choose', 'speech', 'metal', 'heavily', 'flashback', 'basis', 'reference', 'sleazy', 'opened', 'pool', 'meaningful', 'dislike', 'equal', 'crimes', 'kane', 'mafia', 'cruise', 'critic', 'digital', 'regardless', 'ticket', 'neo', 'harder', 'sidekick']\n"
     ]
    }
   ],
   "source": [
    "# create a word2index mapping from word to an integer index\n",
    "word2index = {}\n",
    "ignore_words = find_ignore_words(pos_neg_ratios)\n",
    "vocab_selected = list(set(vocab_selected).difference(set(ignore_words)))\n",
    "for i,word in enumerate(vocab_selected):\n",
    "    if word not in ignore_words:\n",
    "        word2index[word] = i\n",
    "vocab_size = len(word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate .hdf5 files from the processed data\n",
    "Given that the data is moderately large sized, the `hdf5` file format provides a more efficient file representation for further processing. See [here](https://anaconda.org/anaconda/hdf5) for more details and installation instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script once to generate the file \n",
    "# delete the exiting 'data1.hdf5' file before running it again to avoid error \n",
    "labels_train = np.zeros((len(sentiments_train), 2), dtype=int)\n",
    "labels_test = np.zeros((len(sentiments_test), 2), dtype=int)\n",
    "\n",
    "with h5py.File('data1.hdf5', 'w') as hf:\n",
    "    hf.create_dataset('data_train', (labels_train.shape[0], vocab_size), np.int16)\n",
    "    hf.create_dataset('data_test', (labels_test.shape[0], vocab_size), np.int16)\n",
    "    # create training data\n",
    "    for i,(r,l) in enumerate(zip(reviews_train, sentiments_train)):\n",
    "        hf[\"data_train\"][i] = create_input_vector(r,word2index)\n",
    "        # one-hot encoding\n",
    "        if l == 'NEGATIVE':\n",
    "            labels_train[i, 0] = 1\n",
    "        else:\n",
    "            labels_train[i, 1] = 1\n",
    "    # create test data\n",
    "    for i,(r,l) in enumerate(zip(reviews_test, sentiments_test)):\n",
    "        hf[\"data_test\"][i] = create_input_vector(r,word2index)\n",
    "        # one-hot encoding\n",
    "        if l == 'NEGATIVE':\n",
    "            labels_test[i, 0] = 1\n",
    "        else:\n",
    "            labels_test[i, 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf \n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "#tf.compat.v1.random.set_random_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the network\n",
    "learning_rate = 0.01\n",
    "batch_size = 400\n",
    "num_epochs = 100\n",
    "n_input = vocab_size\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vinita\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights and biases in Tensorflow according to the parameters set above\n",
    "#n_hidden_1 = 10  # 1st layer number of neurons\n",
    "#Changing width (number of units)\n",
    "n_hidden_1 = 50\n",
    "n_hidden_2 = 50\n",
    "n_hidden_3 = 50\n",
    "n_hidden_4 = 50\n",
    "n_hidden_5 = 50\n",
    "weights = {\n",
    "\t'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "\t'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "\t'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "\t'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4])),\n",
    "\t'h5': tf.Variable(tf.random_normal([n_hidden_4, n_hidden_5])),\n",
    "\t'out1': tf.Variable(tf.random_normal([n_hidden_5, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "\t'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "\t'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "\t'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "\t'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "\t'b5': tf.Variable(tf.random_normal([n_hidden_5])),\n",
    "\t'out2': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x):\n",
    "    # define the layers of a single layer perceptron\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_2 = tf.nn.sigmoid(tf.matmul(layer_1, weights['h2']) + biases['b2'])\n",
    "    layer_3 = tf.nn.sigmoid(tf.matmul(layer_2, weights['h3']) + biases['b3'])\n",
    "    layer_4 = tf.nn.sigmoid(tf.matmul(layer_3, weights['h4']) + biases['b4'])\n",
    "    layer_5 = tf.nn.sigmoid(tf.matmul(layer_4, weights['h5']) + biases['b5'])\n",
    "    out_layer = tf.nn.sigmoid(tf.matmul(layer_5, weights['out1']) + biases['out2'])\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = multilayer_perceptron(X)\n",
    "# Define loss(softmax_cross_entropy_with_logits) and optimizer(AdamOptimizer)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some macosx installations, conflicting copies of mpilib causes trouble with tensorflow.\n",
    "# use the following two lines to resolve that issue\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.558167, Test_acc: 0.652500\n",
      "Train acc: 0.716667, Test_acc: 0.735000\n",
      "Train acc: 0.775917, Test_acc: 0.765000\n",
      "Train acc: 0.814125, Test_acc: 0.797500\n",
      "Train acc: 0.831208, Test_acc: 0.797500\n",
      "Train acc: 0.841625, Test_acc: 0.807500\n",
      "Train acc: 0.854875, Test_acc: 0.815000\n",
      "Train acc: 0.859542, Test_acc: 0.820000\n",
      "Train acc: 0.869667, Test_acc: 0.812500\n",
      "Train acc: 0.864708, Test_acc: 0.822500\n",
      "Train acc: 0.874000, Test_acc: 0.823750\n",
      "Train acc: 0.877250, Test_acc: 0.823750\n",
      "Train acc: 0.880875, Test_acc: 0.842500\n",
      "Train acc: 0.886875, Test_acc: 0.841250\n",
      "Train acc: 0.889750, Test_acc: 0.852500\n",
      "Train acc: 0.892417, Test_acc: 0.842500\n",
      "Train acc: 0.886875, Test_acc: 0.815000\n",
      "Train acc: 0.885000, Test_acc: 0.837500\n",
      "Train acc: 0.885708, Test_acc: 0.845000\n",
      "Train acc: 0.878042, Test_acc: 0.833750\n",
      "Train acc: 0.882583, Test_acc: 0.847500\n",
      "Train acc: 0.887333, Test_acc: 0.847500\n",
      "Train acc: 0.892583, Test_acc: 0.836250\n",
      "Train acc: 0.894625, Test_acc: 0.850000\n",
      "Train acc: 0.899500, Test_acc: 0.850000\n",
      "Train acc: 0.900083, Test_acc: 0.835000\n",
      "Train acc: 0.900125, Test_acc: 0.858750\n",
      "Train acc: 0.901625, Test_acc: 0.836250\n",
      "Train acc: 0.901167, Test_acc: 0.841250\n",
      "Train acc: 0.901875, Test_acc: 0.843750\n",
      "Train acc: 0.904708, Test_acc: 0.837500\n",
      "Train acc: 0.903792, Test_acc: 0.845000\n",
      "Train acc: 0.907583, Test_acc: 0.842500\n",
      "Train acc: 0.904708, Test_acc: 0.845000\n",
      "Train acc: 0.899792, Test_acc: 0.866250\n",
      "Train acc: 0.904875, Test_acc: 0.852500\n",
      "Train acc: 0.906792, Test_acc: 0.856250\n",
      "Train acc: 0.902875, Test_acc: 0.848750\n",
      "Train acc: 0.908000, Test_acc: 0.863750\n",
      "Train acc: 0.906292, Test_acc: 0.870000\n",
      "Train acc: 0.905042, Test_acc: 0.838750\n",
      "Train acc: 0.894542, Test_acc: 0.838750\n",
      "Train acc: 0.899167, Test_acc: 0.851250\n",
      "Train acc: 0.902500, Test_acc: 0.838750\n",
      "Train acc: 0.903333, Test_acc: 0.847500\n",
      "Train acc: 0.908333, Test_acc: 0.850000\n",
      "Train acc: 0.901292, Test_acc: 0.847500\n",
      "Train acc: 0.908208, Test_acc: 0.856250\n",
      "Train acc: 0.912917, Test_acc: 0.851250\n",
      "Train acc: 0.913417, Test_acc: 0.837500\n",
      "Train acc: 0.900375, Test_acc: 0.847500\n",
      "Train acc: 0.911208, Test_acc: 0.857500\n",
      "Train acc: 0.911750, Test_acc: 0.852500\n",
      "Train acc: 0.910667, Test_acc: 0.856250\n",
      "Train acc: 0.914958, Test_acc: 0.858750\n",
      "Train acc: 0.913292, Test_acc: 0.857500\n",
      "Train acc: 0.913458, Test_acc: 0.853750\n",
      "Train acc: 0.917625, Test_acc: 0.860000\n",
      "Train acc: 0.916417, Test_acc: 0.848750\n",
      "Train acc: 0.917125, Test_acc: 0.867500\n",
      "Train acc: 0.918625, Test_acc: 0.866250\n",
      "Train acc: 0.915583, Test_acc: 0.858750\n",
      "Train acc: 0.912708, Test_acc: 0.845000\n",
      "Train acc: 0.911333, Test_acc: 0.852500\n",
      "Train acc: 0.919708, Test_acc: 0.860000\n",
      "Train acc: 0.917208, Test_acc: 0.865000\n",
      "Train acc: 0.918000, Test_acc: 0.852500\n",
      "Train acc: 0.918458, Test_acc: 0.850000\n",
      "Train acc: 0.908375, Test_acc: 0.857500\n",
      "Train acc: 0.917083, Test_acc: 0.851250\n",
      "Train acc: 0.911417, Test_acc: 0.853750\n",
      "Train acc: 0.918208, Test_acc: 0.861250\n",
      "Train acc: 0.903083, Test_acc: 0.852500\n",
      "Train acc: 0.908458, Test_acc: 0.857500\n",
      "Train acc: 0.914667, Test_acc: 0.860000\n",
      "Train acc: 0.906167, Test_acc: 0.863750\n",
      "Train acc: 0.916542, Test_acc: 0.871250\n",
      "Train acc: 0.914833, Test_acc: 0.861250\n",
      "Train acc: 0.913500, Test_acc: 0.857500\n",
      "Train acc: 0.919917, Test_acc: 0.862500\n",
      "Train acc: 0.920375, Test_acc: 0.860000\n",
      "Train acc: 0.919417, Test_acc: 0.861250\n",
      "Train acc: 0.921250, Test_acc: 0.863750\n",
      "Train acc: 0.920458, Test_acc: 0.875000\n",
      "Train acc: 0.924000, Test_acc: 0.866250\n",
      "Train acc: 0.914875, Test_acc: 0.861250\n",
      "Train acc: 0.920292, Test_acc: 0.867500\n",
      "Train acc: 0.921375, Test_acc: 0.863750\n",
      "Train acc: 0.921292, Test_acc: 0.856250\n",
      "Train acc: 0.917583, Test_acc: 0.867500\n",
      "Train acc: 0.919667, Test_acc: 0.856250\n",
      "Train acc: 0.905750, Test_acc: 0.848750\n",
      "Train acc: 0.919000, Test_acc: 0.858750\n",
      "Train acc: 0.918375, Test_acc: 0.858750\n",
      "Train acc: 0.921208, Test_acc: 0.857500\n",
      "Train acc: 0.924708, Test_acc: 0.860000\n",
      "Train acc: 0.922167, Test_acc: 0.857500\n",
      "Train acc: 0.921750, Test_acc: 0.851250\n",
      "Train acc: 0.924958, Test_acc: 0.861250\n",
      "Train acc: 0.925417, Test_acc: 0.851250\n",
      "Time elapsed - 177.75455904006958 seconds.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "\n",
    "    h = h5py.File('data1.hdf5', 'r')\n",
    "    n1 = h.get('data_train') \n",
    "    n2 = h.get('data_test')\n",
    "\n",
    "    # Training cycle\n",
    "    total_batch_train = int(n1.shape[0] / batch_size)\n",
    "    total_batch_test = int(n2.shape[0] / batch_size)\n",
    "\n",
    "    for iter_num in range(num_epochs):\n",
    "        # variables for train and test accuracies\n",
    "        avg_acc_train = 0.\n",
    "        avg_acc_test = 0.\n",
    "        for i in range(total_batch_train):\n",
    "            train_x = n1[(i) * batch_size: (i + 1) * batch_size, ...]\n",
    "            train_y = labels_train[(i) * batch_size: (i + 1) * batch_size, :]\n",
    "\n",
    "            _, c_train, _logits_train = sess.run([train_op, loss_op, logits], feed_dict={X: train_x, Y: train_y})\n",
    "            _label_train = [np.argmax(i) for i in _logits_train]\n",
    "            _label_train_y = [np.argmax(i) for i in train_y]\n",
    "            _accuracy_train = np.mean(np.array(_label_train) == np.array(_label_train_y))\n",
    "            avg_acc_train += _accuracy_train\n",
    "\n",
    "\n",
    "        for j in range(total_batch_test):\n",
    "            test_x = n2[(j) * batch_size: (j + 1) * batch_size, ...]\n",
    "            test_y = labels_test[(j) * batch_size: (j + 1) * batch_size, :]\n",
    "\n",
    "            c_test, _logits_test = sess.run([loss_op, logits], feed_dict={X: test_x, Y: test_y})\n",
    "            _label_test = [np.argmax(i) for i in _logits_test]\n",
    "            _label_test_y = [np.argmax(i) for i in test_y]\n",
    "            _accuracy_test = np.mean(np.array(_label_test) == np.array(_label_test_y))\n",
    "            avg_acc_test += _accuracy_test\n",
    "\n",
    "        # print the train and test accuracies   \n",
    "        print(\"Train acc: %f, Test_acc: %f\" % (avg_acc_train/total_batch_train, avg_acc_test/total_batch_test))\n",
    "    duration = time.time() - start_time\n",
    "    print('Time elapsed - {} seconds.'.format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
